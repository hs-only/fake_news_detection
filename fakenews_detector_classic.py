# -*- coding: utf-8 -*-
"""fakeNews_detector_classic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MVeXvsnOLTCbRNvazkQZ-Y6Z2w2zHZuT
"""

import numpy as np
import pandas as pd
import itertools
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn
from sklearn.model_selection import train_test_split, KFold, learning_curve
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
from sklearn.metrics import confusion_matrix,precision_recall_curve,average_precision_score
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
import regex as re
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import pickle

true_df=pd.read_csv('/content/True.csv')
false_df=pd.read_csv('/content/Fake.csv')

true_df.head(10)

#quality checking
print("checking missing values....")
print(true_df.isnull().sum())
true_df.info()

false_df.head(10)

false_df.info()

true_df['label']=1
false_df['label']=0

data=pd.concat([true_df,false_df])
data.drop(['text','subject','date'],axis=1,inplace=True)

random_indexes = np.random.randint(0,len(data),len(data))
data = data.iloc[random_indexes].reset_index(drop=True)

#analysis
pd.set_option('display.max_colwidth',500)
rand_=np.random.randint(0,len(data),50)
data.iloc[rand_]

data.info()

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

stop_words = stopwords.words('english')

text_cleaning = r"\b0\S*|\b[^A-Za-z0-9\s]+"

def preprocess_filter(text, stem=False):
  text = re.sub(text_cleaning, " ",str(text.lower()).strip())
  tokens = []
  for token in text.split():
    if token not in stop_words:
      if stem:
        stemmer = SnowballStemmer(language='english')
        token = stemmer.stem(token)
      tokens.append(token)
  return " ".join(tokens)

data['processed_text'] = data['title'].apply(preprocess_filter)
data

sns.countplot(x='label', data=data, palette='hls')

X_train, X_test, Y_train, Y_test = train_test_split(data['processed_text'], data['label'], test_size=0.3, random_state=1)

tfidf_v = TfidfVectorizer()

# Fit and transform the training data
tfidf_X_train = tfidf_v.fit_transform(X_train)

# Transform the test data
tfidf_X_test = tfidf_v.transform(X_test)

# Define the pipeline
logistic_pipe = Pipeline([
    ('tfidf', tfidf_v),  # Use the TfidfVectorizer in the pipeline
    ('logR_Clf', LogisticRegression())
])

# Fit the pipeline on the training data
logistic_pipe.fit(X_train, Y_train)

# Predict on the test data
Y_logR = logistic_pipe.predict(X_test)

# Calculate accuracy
accuracy_logR = sklearn.metrics.accuracy_score(Y_test, Y_logR)
print("Test Accuracy:", accuracy_logR)

cm_log = confusion_matrix(Y_test, Y_logR)

# Plot confusion matrix
def plot_cm(cm,color_map,model_name=''):
  plt.figure(figsize=(8, 6))
  sns.heatmap(cm, annot=True, fmt='d', cmap=color_map, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
  plt.xlabel('Predicted labels ')
  plt.ylabel('True labels')
  plt.title('Confusion Matrix '+model_name)
  plt.show()

plot_cm(cm_log,'coolwarm',':Logistic Regression')

svm_pipeline=Pipeline([('tfidf',tfidf_v),('svm_clf',svm.LinearSVC())])
svm_pipeline.fit(X_train,Y_train)
Y_svm=svm_pipeline.predict(X_test)
accuracy_svm=sklearn.metrics.accuracy_score(Y_test,Y_svm)
print("Test Accuracy:",accuracy_svm)

cm_svm = confusion_matrix(Y_test, Y_svm)
plot_cm(cm_svm,'gist_earth',':SVM')

sgd_pipeline=Pipeline([('tfidf',tfidf_v),('sgdc_clf',SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3,max_iter=1000))])
sgd_pipeline.fit(X_train,Y_train)
Y_sgd=sgd_pipeline.predict(X_test)
accuracy_sgd=sklearn.metrics.accuracy_score(Y_test,Y_sgd)
print("Test Accuracy:",accuracy_sgd)

cm_sgd = confusion_matrix(Y_test, Y_sgd)
plot_cm(cm_sgd,'plasma',':SGD(loss=hinge)')

sgd_log_pipeline=Pipeline([('tfidf',tfidf_v),('sgdc_clf',SGDClassifier(loss='log',penalty='l2',alpha=1e-3,max_iter=1000))])
sgd_log_pipeline.fit(X_train,Y_train)
Y_sgd_log=sgd_log_pipeline.predict(X_test)
accuracy_sgd_log=sklearn.metrics.accuracy_score(Y_test,Y_sgd_log)
print("Test Accuracy:",accuracy_sgd_log)

cm_sgd_log = confusion_matrix(Y_test, Y_sgd_log)
plot_cm(cm_sgd_log,'gist_earth',':SGD(log loss)')

sgd_sl_pipeline=Pipeline([('tfidf',tfidf_v),('sgdc_clf',SGDClassifier(loss='squared_error',penalty='l2',alpha=1e-3,max_iter=1000))])
sgd_sl_pipeline.fit(X_train,Y_train)
Y_sl_sgd=sgd_sl_pipeline.predict(X_test)
accuracy_sgd_sl=sklearn.metrics.accuracy_score(Y_test,Y_sl_sgd)
print("Test Accuracy:",accuracy_sgd_sl)

cm_sgd_sl = confusion_matrix(Y_test, Y_sl_sgd)
plot_cm(cm_sgd_sl,'gist_earth',':SGD(squared loss)')

sgd_3_pipeline=Pipeline([('tfidf',tfidf_v),('sgdc_clf',SGDClassifier(loss='perceptron',penalty='l2',alpha=1e-4,max_iter=1000))])
sgd_3_pipeline.fit(X_train,Y_train)
Y_3_sgd=sgd_3_pipeline.predict(X_test)
accuracy_sgd_3=sklearn.metrics.accuracy_score(Y_test,Y_3_sgd)
print("Test Accuracy:",accuracy_sgd_3)

cm_sgd3 = confusion_matrix(Y_test, Y_3_sgd)
plot_cm(cm_sgd3,'gist_earth',':SGD(percptron loss)')

random_forest=Pipeline([('tfidf',tfidf_v),('rf_clf',RandomForestClassifier(n_estimators=200,n_jobs=3))])
random_forest.fit(X_train,Y_train)
Y_rf=random_forest.predict(X_test)
accuracy_rf=sklearn.metrics.accuracy_score(Y_test,Y_rf)
print("Test accuracy:",accuracy_rf)

cm_rf = confusion_matrix(Y_test, Y_rf)
plot_cm(cm_rf,'gist_earth',':random forest')

# Assuming sgd_pipeline is already defined and trained

# Predict probabilities of the positive class
y_scores = random_forest.predict_proba(X_test)[:,1]

# Compute precision-recall pairs
precision, recall, thresholds = precision_recall_curve(Y_test, y_scores)

# Plot Precision-Recall curve
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve- random forest')
plt.grid(True)
plt.show()

#among all models svm is best
train_sizes, train_scores, test_scores = learning_curve(svm_pipeline, X_train, Y_train, cv=5)
plt.figure()
plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training score')
plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Cross-validation score')
plt.xlabel('Training examples')
plt.ylabel('Score')
plt.title('Learning Curve')
plt.legend()
plt.grid(True)
plt.show()

# Plot precision-recall curve
y_scores_svm = svm_pipeline.decision_function(X_test)
precision, recall, _ = precision_recall_curve(Y_test, y_scores_svm)
plt.figure()
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(True)
plt.show()

with open('svm_pipeline.pkl', 'wb') as f:
    pickle.dump(svm_pipeline, f)

